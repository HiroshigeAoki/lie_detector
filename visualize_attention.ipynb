{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 21:40:25.410168: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-20 21:40:25.410196: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "from transformers import BertJapaneseTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HierBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoki/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'defaults.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      "  name: HierBERT\n",
      "  model:\n",
      "    num_labels: 2\n",
      "    _target_: src.model.HierBERT.HierchicalBERT\n",
      "    use_ave_pooled_output: true\n",
      "    output_attentions: true\n",
      "    is_japanese: true\n",
      "  tokenizer:\n",
      "    _target_: src.tokenizer.tokenizer_HierBERT.HierBertTokenizer\n",
      "    sent_length: 256\n",
      "    doc_length: 256\n",
      "    pretrained_model: cl-tohoku/bert-base-japanese-v2\n",
      "  data_module:\n",
      "    _target_: src.model.HierBERTDataModule.CreateHierBertDataModule\n",
      "    batch_size: 64\n",
      "  sent_level_BERT_config:\n",
      "    _target_: transformers.BertConfig\n",
      "    hidden_size: 768\n",
      "    num_hidden_layers: 12\n",
      "    num_attention_heads: 12\n",
      "data:\n",
      "  name: wereWolf_sample\n",
      "  dir: /disk/ssd14tb/haoki/Documents/vscode-workplaces/lie_detector/data/nested_sample/\n",
      "optim:\n",
      "  name: AdamW\n",
      "  optimizer:\n",
      "    _target_: torch.optim.AdamW\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.01\n",
      "experiment: predict\n",
      "name: ave_pooled_base-v2\n",
      "message: null\n",
      "trainer:\n",
      "  accumulate_grad_batches: 1\n",
      "  benchmark: true\n",
      "  deterministic: true\n",
      "  fast_dev_run: false\n",
      "  gpus:\n",
      "  - 6\n",
      "  max_epochs: 10\n",
      "  overfit_batches: 0.0\n",
      "  precision: 16\n",
      "  terminate_on_nan: true\n",
      "early_stopping:\n",
      "  _target_: pytorch_lightning.callbacks.EarlyStopping\n",
      "  monitor: val_loss\n",
      "  min_delta: 0.005\n",
      "  patience: 3\n",
      "  mode: min\n",
      "  check_on_train_epoch_end: false\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "abs_data_path = os.path.abspath(\"data/nested_sample/\")\n",
    "\n",
    "with hydra.initialize(config_path='config'):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"defaults.yaml\",\n",
    "        overrides=[\n",
    "            \"experiment=predict\",\n",
    "            \"name=ave_pooled_base-v2\",\n",
    "            \"model=HierBERT\",\n",
    "            \"data=wereWolf_sample\",\n",
    "            f\"data.dir={abs_data_path}/\",\n",
    "            \"trainer.gpus=[6]\",\n",
    "            \"model.tokenizer.pretrained_model=cl-tohoku/bert-base-japanese-v2\",\n",
    "            \"model.sent_level_BERT_config.hidden_size=768\",\n",
    "            \"model.sent_level_BERT_config.num_hidden_layers=12\",\n",
    "            \"model.sent_level_BERT_config.num_attention_heads=12\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "data_module = hydra.utils.instantiate(\n",
    "    cfg.model.data_module,\n",
    "    data_dir=cfg.data.dir,\n",
    "    tokenizer=cfg.model.tokenizer,\n",
    "    _recursive_=False,\n",
    ")\n",
    "\n",
    "model = hydra.utils.instantiate(\n",
    "        cfg.model.model,\n",
    "        pretrained_model=cfg.model.tokenizer.pretrained_model,\n",
    "        sent_level_BERT_config=cfg.model.sent_level_BERT_config,\n",
    "        optim=cfg.optim,\n",
    "        _recursive_=False,\n",
    ")\n",
    "\n",
    "#tb_logger = pl.loggers.TensorBoardLogger(\".\", \"\", \"\", log_graph=True, default_hp_metric=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    **OmegaConf.to_container(cfg.trainer),\n",
    "#    callbacks=[tb_logger],\n",
    "    plugins=DDPPlugin(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/wereWolf_sample/HierBERT/baseline/ave_pooled_base-v2/checkpoints/epoch=2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoki/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/torch/cuda/__init__.py:106: UserWarning: \n",
      "A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the A100-PCIE-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b56dc5eafaa4b17bcb2c07b95779b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_epoch = 2\n",
    "ckpt_path = f'outputs/{cfg.data.name}/{cfg.model.name}/baseline/{cfg.name}/checkpoints/epoch={best_epoch}.ckpt'\n",
    "print(ckpt_path)\n",
    "outputs = trainer.predict(model=model, datamodule=data_module, ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.cat([p['logits'] for p in outputs], dim=0)\n",
    "word_attentions = torch.cat([torch.stack(p['word_attentions']).permute(1, 0, 2) for p in outputs])\n",
    "sent_attentions = torch.cat([p['sent_attentions'] for p in outputs])\n",
    "input_ids = torch.cat([p['input_ids'] for p in outputs])\n",
    "labels = torch.cat([p['labels'] for p in outputs])\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(f'{cfg.model.tokenizer.pretrained_model}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ヒートマップを作成中...: 256it [00:00, 1632.30it/s]\n",
      "ヒートマップを作成中...: 256it [00:00, 989.50it/s]\n",
      "ヒートマップを作成中...: 256it [00:00, 2373.89it/s]\n",
      "ヒートマップを作成中...: 256it [00:00, 1664.56it/s]\n",
      "ヒートマップを作成中...: 256it [00:00, 991.89it/s]\n",
      "ヒートマップを作成中...: 256it [00:00, 1004.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.visualization.plot_attention import plot_word_attentions\n",
    "\n",
    "ploted_doc = []\n",
    "for _input_ids, _word_attentions in zip(input_ids, word_attentions):\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in _input_ids]\n",
    "    ploted_doc.append(plot_word_attentions(doc=tokens, weights_list=_word_attentions, threshold=0.01, size=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "display(HTML(ploted_doc[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_sent_attention() got an unexpected keyword argument 'weights_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_81381/3548733782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sent_attentions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_input_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mploted_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_sent_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_sent_attentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: plot_sent_attention() got an unexpected keyword argument 'weights_list'"
     ]
    }
   ],
   "source": [
    "from src.visualization.plot_attention import plot_sent_attention\n",
    "\n",
    "ploted_doc = []\n",
    "for _input_ids, _sent_attentions in zip(input_ids, sent_attentions):\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in _input_ids]\n",
    "    ploted_doc.append(plot_sent_attention(doc=tokens, weights_list=_sent_attentions, threshold=0.01, size=3))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fb0056378317a9ced497e21f9ad1030b78be77748927f9cf06c62d2ffd6c3d0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

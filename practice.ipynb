{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "a7b981ef8d205b9c9d0b9ff0462adc75b1bd46e55cd2e510076f03f58ed43f06"
   }
  },
  "interpreter": {
   "hash": "7d5b112f5c4cb3ba1f0eba8711ea2a946c1ba463adebf4c3d9a6d0e4244602f3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## bert_wordattention.py実装"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from project.model.hierarchical_BERTDataModule import CreateHierBertDataModule\n",
    "datamodule = CreateHierBertDataModule(data_dir='data/nested_sample/', max_len_sent=160, max_len_doc=170, batch_size=16)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "datamodule.setup()\n",
    "test_loder = datamodule.test_dataloader()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "batch = next(iter(test_loder))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "input_ids = batch['input_ids'].permute(1,0,2)\n",
    "attention_mask = batch['attention_mask'].permute(1,0,2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "print(input_ids.name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "next(iter(input_ids)).shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([6, 160])"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "input_ids.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([170, 6, 160])"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "from project.tokenizer.tokenizer_hierBERT import hierBertTokenizer\n",
    "from transformers import BertModel, BertJapaneseTokenizer\n",
    "pretrained_model='cl-tohoku/bert-large-japanese'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model, additional_special_tokens=['<person>'])\n",
    "bert = BertModel.from_pretrained(\n",
    "            pretrained_model\n",
    "        )\n",
    "bert.resize_token_embeddings(len(tokenizer))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Embedding(32769, 1024)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "i = next(iter(input_ids))\n",
    "a = next(iter(attention_mask))\n",
    "output = bert(i, a)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "output.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output', 'attentions'])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "last_hidden_state, pooler_output = [], []\n",
    "for _input_ids, _attention_mask in zip(input_ids, attention_mask):\n",
    "    _last_hidden_state, _pooler_output = bert(input_ids=_input_ids, attention_mask=_attention_mask).values()\n",
    "    last_hidden_state.append(_last_hidden_state)\n",
    "    pooler_output.append(_pooler_output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "print(pooler_output[0].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([6, 1024])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "pooler_output[0][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.0167, -0.4603,  0.3062,  ..., -0.9941,  0.0714, -0.3944],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "print((1,2,3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 2, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "input_ids = torch.stack(pooler_output).permute(1, 0, 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "torch.ones_like(input_ids).shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([6, 170, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "if torch.ones_like(input_ids).shape == (6,170,1024):\n",
    "    print('yay')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "yay\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "pretrained_model: str = 'cl-tohoku/bert-large-japanese'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model, additional_special_tokens=['<person>'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "bert.config"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"cl-tohoku/bert-large-japanese\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"output_attentions\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
       "  \"transformers_version\": \"4.10.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32768\n",
       "}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "bert.embeddings.word_embeddings.num_embeddings"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "bert.embeddings.word_embeddings.embedding_dim"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import torch\n",
    "bert.embeddings.word_embeddings(torch.tensor(32767))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.0180, -0.0543, -0.0301,  ...,  0.0236, -0.0200, -0.0463],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "tokenizer.vocab_size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "tokenizer.convert_tokens_to_ids('<person>')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from project.tokenizer.tokenizer_hierBERT import hierBertTokenizer\n",
    "from transformers import BertModel, BertJapaneseTokenizer\n",
    "pretrained_model='cl-tohoku/bert-large-japanese'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model, additional_special_tokens=['<person>'])\n",
    "bert = BertModel.from_pretrained(\n",
    "            pretrained_model, output_attentions=True,\n",
    "        )\n",
    "bert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer = hierBertTokenizer(max_len_sent=160, max_len_doc=160)\n",
    "import pandas as pd\n",
    "\n",
    "test = pd.read_pickle('data/nested_sample/test.pkl')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "input_ids, attention_mask = tokenizer.encode(test.loc[:,'nested_utters'].iloc[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "attention_mask.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([160, 160])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# input_ids, _, attention_mask = tokenizer.encode_plus('こんにちは、僕は青木ですははは', return_tensors='pt').values()\n",
    "th = 9\n",
    "last_hidden_state, pooler_output, attentions = bert(input_ids[0:th], attention_mask[0:th]).values()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "pooler_output.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([9, 1024])"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_pickle('data/nested_sample/test.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from project.tokenizer.tokenizer_hierBERT import hierBertTokenizer\n",
    "\n",
    "tokenizer = hierBertTokenizer(max_len_sent=512)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from transformers import BertModel\n",
    "pretrained_model='cl-tohoku/bert-large-japanese'\n",
    "bert = BertModel.from_pretrained(\n",
    "            pretrained_model\n",
    "        )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "input_ids, attention_mask = tokenizer.encode(test.loc[:,'nested_utters'].iloc[0].values.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "input_ids.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([73, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "last_hidden_state, pooler_output, attentions = bert(*tokenizer.encode(test.loc[:,'nested_utters'].iloc[0].values.tolist())).values()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_110565/3851646739.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooler_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nested_utters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    984\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "attentions[-1].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 8, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "from transformers import BertModel\n",
    "\n",
    "class WordAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            recurrent_size: int,\n",
    "            attention_dim: int,\n",
    "            bert_version: str = \"bert-base-uncased\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attention_dim = attention_dim\n",
    "        self.recurrent_size = recurrent_size\n",
    "        self.bert_model = BertModel.from_pretrained(bert_version)\n",
    "\n",
    "        # Maps BERT output to `attention_dim` sized tensor\n",
    "        self.word_weight = nn.Linear(self.recurrent_size, self.attention_dim)\n",
    "\n",
    "        # Word context vector (u_w) to take dot-product with\n",
    "        self.context_weight = nn.Linear(self.attention_dim, 1)\n",
    "\n",
    "    def recurrent_size(self):\n",
    "        return self.recurrent_size\n",
    "\n",
    "    def forward(self, docs, doc_lengths, sent_lengths, attention_masks, token_type_ids):\n",
    "        \"\"\"\n",
    "        :param docs: encoded document-level data; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
    "        :param doc_lengths: unpadded document lengths; LongTensor (num_docs)\n",
    "        :param sent_lengths: unpadded sentence lengths; LongTensor (num_docs, max_sent_len)\n",
    "        :param attention_masks: BERT attention masks; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
    "        :param token_type_ids: BERT token type IDs; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
    "        :return: sentences embeddings, docs permutation indices, docs batch sizes, word attention weights\n",
    "        \"\"\"\n",
    "\n",
    "        # Sort documents by decreasing order in length\n",
    "        doc_lengths, doc_perm_idx = doc_lengths.sort(dim=0, descending=True)\n",
    "        docs = docs[doc_perm_idx]\n",
    "        sent_lengths = sent_lengths[doc_perm_idx]\n",
    "\n",
    "        # Make a long batch of sentences by removing pad-sentences\n",
    "        # i.e. `docs` was of size (num_docs, padded_doc_length, padded_sent_length)\n",
    "        # -> `packed_sents.data` is now of size (num_sents, padded_sent_length)\n",
    "        packed_sents = pack_padded_sequence(docs, lengths=doc_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        # effective batch size at each timestep\n",
    "        docs_valid_bsz = packed_sents.batch_sizes\n",
    "\n",
    "        # Make a long batch of sentence lengths by removing pad-sentences\n",
    "        # i.e. `sent_lengths` was of size (num_docs, padded_doc_length)\n",
    "        # -> `packed_sent_lengths.data` is now of size (num_sents)\n",
    "        packed_sent_lengths = pack_padded_sequence(sent_lengths, lengths=doc_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        # Make a long batch of attention masks by removing pad-sentences\n",
    "        # i.e. `docs` was of size (num_docs, padded_doc_length, padded_sent_length)\n",
    "        # -> `packed_attention_masks.data` is now of size (num_sents, padded_sent_length)\n",
    "        packed_attention_masks = pack_padded_sequence(attention_masks, lengths=doc_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        # Make a long batch of token_type_ids by removing pad-sentences\n",
    "        # i.e. `docs` was of size (num_docs, padded_doc_length, padded_sent_length)\n",
    "        # -> `token_type_ids.data` is now of size (num_sents, padded_sent_length)\n",
    "        packed_token_type_ids = pack_padded_sequence(token_type_ids, lengths=doc_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        sents, sent_lengths, attn_masks, token_types = (\n",
    "            packed_sents.data, packed_sent_lengths.data, packed_attention_masks.data, packed_token_type_ids.data\n",
    "        )\n",
    "\n",
    "        # Sort sents by decreasing order in sentence lengths\n",
    "        sent_lengths, sent_perm_idx = sent_lengths.sort(dim=0, descending=True)\n",
    "        sents = sents[sent_perm_idx]\n",
    "\n",
    "        embeddings, pooled_out = self.bert_model(sents, attention_mask=attn_masks, token_type_ids=token_types)\n",
    "\n",
    "        packed_words = pack_padded_sequence(embeddings, lengths=sent_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        # effective batch size at each timestep\n",
    "        sentences_valid_bsz = packed_words.batch_sizes\n",
    "\n",
    "        u_i = torch.tanh(self.word_weight(packed_words.data))\n",
    "        u_w = self.context_weight(u_i).squeeze(1)\n",
    "        val = u_w.max()\n",
    "        att = torch.exp(u_w - val)\n",
    "\n",
    "        # Restore as sentences by repadding\n",
    "        att, _ = pad_packed_sequence(PackedSequence(att, sentences_valid_bsz), batch_first=True)\n",
    "\n",
    "        att_weights = att / torch.sum(att, dim=1, keepdim=True)\n",
    "\n",
    "        # Restore as sentences by repadding\n",
    "        sents, _ = pad_packed_sequence(packed_words, batch_first=True)\n",
    "\n",
    "        sents = sents * att_weights.unsqueeze(2)\n",
    "        sents = sents.sum(dim=1)\n",
    "\n",
    "        # Restore the original order of sentences (undo the first sorting)\n",
    "        _, sent_unperm_idx = sent_perm_idx.sort(dim=0, descending=False)\n",
    "        sents = sents[sent_unperm_idx]\n",
    "\n",
    "        att_weights = att_weights[sent_unperm_idx]\n",
    "\n",
    "        return sents, doc_perm_idx, docs_valid_bsz, att_weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_pickle(\"project/model/data/nested_sample/train.pkl\")\n",
    "from collections import Counter\n",
    "import MeCab\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "tagger = MeCab.Tagger(\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "\n",
    "def pos_count(df: pd.DataFrame):\n",
    "    df.loc[:, 'raw_nested_utters'] = df.loc[:, 'raw_nested_utters'].apply(tagger.parse)\n",
    "    for _, line in df.loc[:, 'raw_nested_utters'].items():\n",
    "        pos = [mor.split(\"\\t\")[1].split(\",\")[0] for mor in line.split(\"\\n\")[:-2]]\n",
    "        pos_count = Counter(pos)\n",
    "        df['noun'] = pos_count['名詞']\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "a = pd.DataFrame()\n",
    "a.append({'noun':2, 'adjective': 3})\n",
    "print(a)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Can only append a dict if ignore_index=True",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_71494/2220349107.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'noun'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adjective'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   8925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8926\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8927\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only append a dict if ignore_index=True\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8928\u001b[0m                 \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can only append a dict if ignore_index=True"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print('a')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "train.loc[:,'nested_utters'] = train.loc[:,'nested_utters'].parallel_apply(pos_count)\n",
    "train.loc[:,'nested_utters'].head()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'parallel_apply'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66986/1834561300.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nested_utters'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nested_utters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nested_utters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'parallel_apply'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 形態素の種類別の数を計算しておく。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_pickle(\"project/model/data/nested_sample/test.pkl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "test.loc[:,\"nested_utters\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                                        raw_nested...\n",
       "1                                         raw_neste...\n",
       "2                                        raw_nested...\n",
       "3                                        raw_nested...\n",
       "4                                         raw_neste...\n",
       "5                                         raw_neste...\n",
       "Name: nested_utters, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "test.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nested_utters</th>\n",
       "      <th>num_utters</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw_nested...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raw_neste...</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raw_nested...</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raw_nested...</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raw_neste...</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       nested_utters  num_utters  labels\n",
       "0                                      raw_nested...          73       0\n",
       "1                                       raw_neste...         116       1\n",
       "2                                      raw_nested...          60       0\n",
       "3                                      raw_nested...          77       0\n",
       "4                                       raw_neste...         116       1"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(list(range(0,100,10)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_morphemes = pd.concat([nested.loc[:,'num_morphemes'] for nested in test.loc[:,\"nested_utters\"]])\n",
    "fig = num_morphemes.hist(bins=100, grid=True, figsize=(30,10), xrot=20, xlabelsize=16, ylabelsize=16).set_xticks(ticks=list(range(0,150,3)))\n",
    "#fig.figure.savefig(\"num_utter_hist.png\")\n",
    "display(fig[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "test.loc[:,\"nested_utters\"][0].head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_nested_utters</th>\n",
       "      <th>parsed_nested_utters</th>\n",
       "      <th>num_morphemes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>昼間も議論が活発なのだな。議題に答えておこう。1.思うところもあるだろうし本人に任せる。2....</td>\n",
       "      <td>表層形   品詞  品詞細分類1 品詞細分類2 品詞細分類3       活用型  ...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>それから霊能者の即時CO纏め役というのはやりたくない。どうせ対抗が出るのだ。そうなれば結局共...</td>\n",
       "      <td>表層形   品詞  品詞細分類1 品詞細分類2 品詞細分類3       活用型 ...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23:00まで待つというのもそれまでの時間が勿体無いな。共有者も出ないようなら私が纏め役をや...</td>\n",
       "      <td>表層形   品詞        品詞細分類1 品詞細分類2 品詞細分類3    ...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>条件も提示しておこう。1.私を今日の占対象にしない。2.占いは今日から黒狙い。3.私は3日目...</td>\n",
       "      <td>表層形  品詞 品詞細分類1 品詞細分類2 品詞細分類3    活用型   活用形...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>過激、か。確かにそうかもしれんな。だがあまり話す事がないとは言え共有者や纏め役についてでだら...</td>\n",
       "      <td>表層形   品詞        品詞細分類1 品詞細分類2 品詞細分類3       ...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   raw_nested_utters  \\\n",
       "0  昼間も議論が活発なのだな。議題に答えておこう。1.思うところもあるだろうし本人に任せる。2....   \n",
       "1  それから霊能者の即時CO纏め役というのはやりたくない。どうせ対抗が出るのだ。そうなれば結局共...   \n",
       "2  23:00まで待つというのもそれまでの時間が勿体無いな。共有者も出ないようなら私が纏め役をや...   \n",
       "3  条件も提示しておこう。1.私を今日の占対象にしない。2.占いは今日から黒狙い。3.私は3日目...   \n",
       "4  過激、か。確かにそうかもしれんな。だがあまり話す事がないとは言え共有者や纏め役についてでだら...   \n",
       "\n",
       "                                parsed_nested_utters  num_morphemes  \n",
       "0      表層形   品詞  品詞細分類1 品詞細分類2 品詞細分類3       活用型  ...             87  \n",
       "1       表層形   品詞  品詞細分類1 品詞細分類2 品詞細分類3       活用型 ...             54  \n",
       "2        表層形   品詞        品詞細分類1 品詞細分類2 品詞細分類3    ...             42  \n",
       "3       表層形  品詞 品詞細分類1 品詞細分類2 品詞細分類3    活用型   活用形...             73  \n",
       "4     表層形   品詞        品詞細分類1 品詞細分類2 品詞細分類3       ...             72  "
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "test.loc[:,\"nested_utters\"][0].loc[:,\"parsed_nested_utters\"].head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0             表層形   品詞  品詞細分類1 品詞細分類2 品詞細分類3    活用型...\n",
       "1         表層形   品詞 品詞細分類1 品詞細分類2 品詞細分類3   活用型   活用形...\n",
       "2          表層形   品詞 品詞細分類1 品詞細分類2 品詞細分類3   活用型   活用...\n",
       "3        表層形   品詞  品詞細分類1 品詞細分類2 品詞細分類3       活用型  ...\n",
       "4       表層形   品詞 品詞細分類1 品詞細分類2 品詞細分類3   活用型   活用形  ...\n",
       "Name: parsed_nested_utters, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "test.loc[:,\"nested_utters\"][0].loc[:,\"parsed_nested_utters\"][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>表層形</th>\n",
       "      <th>品詞</th>\n",
       "      <th>品詞細分類1</th>\n",
       "      <th>品詞細分類2</th>\n",
       "      <th>品詞細分類3</th>\n",
       "      <th>活用型</th>\n",
       "      <th>活用形</th>\n",
       "      <th>原形</th>\n",
       "      <th>読み</th>\n",
       "      <th>発音</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ういうい</td>\n",
       "      <td>名詞</td>\n",
       "      <td>固有名詞</td>\n",
       "      <td>一般</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ういうい</td>\n",
       "      <td>ウイウイ</td>\n",
       "      <td>ウイウイ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>、</td>\n",
       "      <td>記号</td>\n",
       "      <td>読点</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>、</td>\n",
       "      <td>、</td>\n",
       "      <td>、</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>帰っ</td>\n",
       "      <td>動詞</td>\n",
       "      <td>自立</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>五段・ラ行</td>\n",
       "      <td>連用タ接続</td>\n",
       "      <td>帰る</td>\n",
       "      <td>カエッ</td>\n",
       "      <td>カエッ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>て</td>\n",
       "      <td>助詞</td>\n",
       "      <td>接続助詞</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>て</td>\n",
       "      <td>テ</td>\n",
       "      <td>テ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>き</td>\n",
       "      <td>動詞</td>\n",
       "      <td>非自立</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>カ変・クル</td>\n",
       "      <td>連用形</td>\n",
       "      <td>くる</td>\n",
       "      <td>キ</td>\n",
       "      <td>キ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>た</td>\n",
       "      <td>助動詞</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>特殊・タ</td>\n",
       "      <td>基本形</td>\n",
       "      <td>た</td>\n",
       "      <td>タ</td>\n",
       "      <td>タ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ぜ</td>\n",
       "      <td>助詞</td>\n",
       "      <td>終助詞</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ぜ</td>\n",
       "      <td>ゼ</td>\n",
       "      <td>ゼ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>。</td>\n",
       "      <td>記号</td>\n",
       "      <td>句点</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>。</td>\n",
       "      <td>。</td>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;person&gt;</td>\n",
       "      <td>名詞</td>\n",
       "      <td>固有名詞</td>\n",
       "      <td>人名</td>\n",
       "      <td>名</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>組長</td>\n",
       "      <td>名詞</td>\n",
       "      <td>一般</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>組長</td>\n",
       "      <td>クミチョウ</td>\n",
       "      <td>クミチョー</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>の</td>\n",
       "      <td>助詞</td>\n",
       "      <td>連体化</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>の</td>\n",
       "      <td>ノ</td>\n",
       "      <td>ノ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>お</td>\n",
       "      <td>接頭詞</td>\n",
       "      <td>名詞接続</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>お</td>\n",
       "      <td>オ</td>\n",
       "      <td>オ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>帰り</td>\n",
       "      <td>名詞</td>\n",
       "      <td>一般</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>帰り</td>\n",
       "      <td>カエリ</td>\n",
       "      <td>カエリ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>だ</td>\n",
       "      <td>助動詞</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>特殊・ダ</td>\n",
       "      <td>基本形</td>\n",
       "      <td>だ</td>\n",
       "      <td>ダ</td>\n",
       "      <td>ダ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>。</td>\n",
       "      <td>記号</td>\n",
       "      <td>句点</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>。</td>\n",
       "      <td>。</td>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>若頭</td>\n",
       "      <td>名詞</td>\n",
       "      <td>固有名詞</td>\n",
       "      <td>一般</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>若頭</td>\n",
       "      <td>ワカガシラ</td>\n",
       "      <td>ワカガシラ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;person&gt;</td>\n",
       "      <td>名詞</td>\n",
       "      <td>固有名詞</td>\n",
       "      <td>人名</td>\n",
       "      <td>名</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>と</td>\n",
       "      <td>助詞</td>\n",
       "      <td>並立助詞</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>と</td>\n",
       "      <td>ト</td>\n",
       "      <td>ト</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>特攻隊長</td>\n",
       "      <td>名詞</td>\n",
       "      <td>固有名詞</td>\n",
       "      <td>一般</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>特攻隊長</td>\n",
       "      <td>トッコウタイチョウ</td>\n",
       "      <td>トッコータイチョー</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;person&gt;</td>\n",
       "      <td>名詞</td>\n",
       "      <td>固有名詞</td>\n",
       "      <td>人名</td>\n",
       "      <td>名</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>（</td>\n",
       "      <td>記号</td>\n",
       "      <td>括弧開</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>（</td>\n",
       "      <td>（</td>\n",
       "      <td>（</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>勝手</td>\n",
       "      <td>名詞</td>\n",
       "      <td>形容動詞語幹</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>勝手</td>\n",
       "      <td>カッテ</td>\n",
       "      <td>カッテ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>に</td>\n",
       "      <td>助詞</td>\n",
       "      <td>副詞化</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>に</td>\n",
       "      <td>ニ</td>\n",
       "      <td>ニ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>認定</td>\n",
       "      <td>名詞</td>\n",
       "      <td>サ変接続</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>認定</td>\n",
       "      <td>ニンテイ</td>\n",
       "      <td>ニンテイ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>）</td>\n",
       "      <td>記号</td>\n",
       "      <td>括弧閉</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>）</td>\n",
       "      <td>）</td>\n",
       "      <td>）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>は</td>\n",
       "      <td>助詞</td>\n",
       "      <td>係助詞</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>は</td>\n",
       "      <td>ハ</td>\n",
       "      <td>ワ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>まだ</td>\n",
       "      <td>副詞</td>\n",
       "      <td>助詞類接続</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>まだ</td>\n",
       "      <td>マダ</td>\n",
       "      <td>マダ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>来</td>\n",
       "      <td>動詞</td>\n",
       "      <td>自立</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>カ変・来ル</td>\n",
       "      <td>連用形</td>\n",
       "      <td>来る</td>\n",
       "      <td>キ</td>\n",
       "      <td>キ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>て</td>\n",
       "      <td>動詞</td>\n",
       "      <td>非自立</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>一段</td>\n",
       "      <td>未然形</td>\n",
       "      <td>てる</td>\n",
       "      <td>テ</td>\n",
       "      <td>テ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ない</td>\n",
       "      <td>助動詞</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>特殊・ナイ</td>\n",
       "      <td>基本形</td>\n",
       "      <td>ない</td>\n",
       "      <td>ナイ</td>\n",
       "      <td>ナイ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>よう</td>\n",
       "      <td>名詞</td>\n",
       "      <td>非自立</td>\n",
       "      <td>助動詞語幹</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>よう</td>\n",
       "      <td>ヨウ</td>\n",
       "      <td>ヨー</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>だ</td>\n",
       "      <td>助動詞</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>特殊・ダ</td>\n",
       "      <td>基本形</td>\n",
       "      <td>だ</td>\n",
       "      <td>ダ</td>\n",
       "      <td>ダ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>な</td>\n",
       "      <td>助詞</td>\n",
       "      <td>終助詞</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>な</td>\n",
       "      <td>ナ</td>\n",
       "      <td>ナ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>。</td>\n",
       "      <td>記号</td>\n",
       "      <td>句点</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>。</td>\n",
       "      <td>。</td>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>議事録</td>\n",
       "      <td>名詞</td>\n",
       "      <td>固有名詞</td>\n",
       "      <td>一般</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>議事録</td>\n",
       "      <td>ギジロク</td>\n",
       "      <td>ギジロク</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>を</td>\n",
       "      <td>助詞</td>\n",
       "      <td>格助詞</td>\n",
       "      <td>一般</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>を</td>\n",
       "      <td>ヲ</td>\n",
       "      <td>ヲ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>読ん</td>\n",
       "      <td>動詞</td>\n",
       "      <td>自立</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>五段・マ行</td>\n",
       "      <td>連用タ接続</td>\n",
       "      <td>読む</td>\n",
       "      <td>ヨン</td>\n",
       "      <td>ヨン</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>で</td>\n",
       "      <td>助詞</td>\n",
       "      <td>接続助詞</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>で</td>\n",
       "      <td>デ</td>\n",
       "      <td>デ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>くる</td>\n",
       "      <td>動詞</td>\n",
       "      <td>非自立</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>カ変・クル</td>\n",
       "      <td>基本形</td>\n",
       "      <td>くる</td>\n",
       "      <td>クル</td>\n",
       "      <td>クル</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ぜ</td>\n",
       "      <td>助詞</td>\n",
       "      <td>終助詞</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ぜ</td>\n",
       "      <td>ゼ</td>\n",
       "      <td>ゼ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>。</td>\n",
       "      <td>記号</td>\n",
       "      <td>句点</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>。</td>\n",
       "      <td>。</td>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         表層形   品詞  品詞細分類1 品詞細分類2 品詞細分類3    活用型    活用形    原形         読み  \\\n",
       "0       ういうい   名詞    固有名詞     一般   None   None   None  ういうい       ウイウイ   \n",
       "1          、   記号      読点   None   None   None   None     、          、   \n",
       "2         帰っ   動詞      自立   None   None  五段・ラ行  連用タ接続    帰る        カエッ   \n",
       "3          て   助詞    接続助詞   None   None   None   None     て          テ   \n",
       "4          き   動詞     非自立   None   None  カ変・クル    連用形    くる          キ   \n",
       "5          た  助動詞    None   None   None   特殊・タ    基本形     た          タ   \n",
       "6          ぜ   助詞     終助詞   None   None   None   None     ぜ          ゼ   \n",
       "7          。   記号      句点   None   None   None   None     。          。   \n",
       "8   <person>   名詞    固有名詞     人名      名   None   None  None       None   \n",
       "9         組長   名詞      一般   None   None   None   None    組長      クミチョウ   \n",
       "10         の   助詞     連体化   None   None   None   None     の          ノ   \n",
       "11         お  接頭詞    名詞接続   None   None   None   None     お          オ   \n",
       "12        帰り   名詞      一般   None   None   None   None    帰り        カエリ   \n",
       "13         だ  助動詞    None   None   None   特殊・ダ    基本形     だ          ダ   \n",
       "14         。   記号      句点   None   None   None   None     。          。   \n",
       "15        若頭   名詞    固有名詞     一般   None   None   None    若頭      ワカガシラ   \n",
       "16  <person>   名詞    固有名詞     人名      名   None   None  None       None   \n",
       "17         と   助詞    並立助詞   None   None   None   None     と          ト   \n",
       "18      特攻隊長   名詞    固有名詞     一般   None   None   None  特攻隊長  トッコウタイチョウ   \n",
       "19  <person>   名詞    固有名詞     人名      名   None   None  None       None   \n",
       "20         （   記号     括弧開   None   None   None   None     （          （   \n",
       "21        勝手   名詞  形容動詞語幹   None   None   None   None    勝手        カッテ   \n",
       "22         に   助詞     副詞化   None   None   None   None     に          ニ   \n",
       "23        認定   名詞    サ変接続   None   None   None   None    認定       ニンテイ   \n",
       "24         ）   記号     括弧閉   None   None   None   None     ）          ）   \n",
       "25         は   助詞     係助詞   None   None   None   None     は          ハ   \n",
       "26        まだ   副詞   助詞類接続   None   None   None   None    まだ         マダ   \n",
       "27         来   動詞      自立   None   None  カ変・来ル    連用形    来る          キ   \n",
       "28         て   動詞     非自立   None   None     一段    未然形    てる          テ   \n",
       "29        ない  助動詞    None   None   None  特殊・ナイ    基本形    ない         ナイ   \n",
       "30        よう   名詞     非自立  助動詞語幹   None   None   None    よう         ヨウ   \n",
       "31         だ  助動詞    None   None   None   特殊・ダ    基本形     だ          ダ   \n",
       "32         な   助詞     終助詞   None   None   None   None     な          ナ   \n",
       "33         。   記号      句点   None   None   None   None     。          。   \n",
       "34       議事録   名詞    固有名詞     一般   None   None   None   議事録       ギジロク   \n",
       "35         を   助詞     格助詞     一般   None   None   None     を          ヲ   \n",
       "36        読ん   動詞      自立   None   None  五段・マ行  連用タ接続    読む         ヨン   \n",
       "37         で   助詞    接続助詞   None   None   None   None     で          デ   \n",
       "38        くる   動詞     非自立   None   None  カ変・クル    基本形    くる         クル   \n",
       "39         ぜ   助詞     終助詞   None   None   None   None     ぜ          ゼ   \n",
       "40         。   記号      句点   None   None   None   None     。          。   \n",
       "\n",
       "           発音  \n",
       "0        ウイウイ  \n",
       "1           、  \n",
       "2         カエッ  \n",
       "3           テ  \n",
       "4           キ  \n",
       "5           タ  \n",
       "6           ゼ  \n",
       "7           。  \n",
       "8        None  \n",
       "9       クミチョー  \n",
       "10          ノ  \n",
       "11          オ  \n",
       "12        カエリ  \n",
       "13          ダ  \n",
       "14          。  \n",
       "15      ワカガシラ  \n",
       "16       None  \n",
       "17          ト  \n",
       "18  トッコータイチョー  \n",
       "19       None  \n",
       "20          （  \n",
       "21        カッテ  \n",
       "22          ニ  \n",
       "23       ニンテイ  \n",
       "24          ）  \n",
       "25          ワ  \n",
       "26         マダ  \n",
       "27          キ  \n",
       "28          テ  \n",
       "29         ナイ  \n",
       "30         ヨー  \n",
       "31          ダ  \n",
       "32          ナ  \n",
       "33          。  \n",
       "34       ギジロク  \n",
       "35          ヲ  \n",
       "36         ヨン  \n",
       "37          デ  \n",
       "38         クル  \n",
       "39          ゼ  \n",
       "40          。  "
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "test.loc[:,\"nested_utters\"][0].loc[:,\"parsed_nested_utters\"][0].loc[:,\"品詞\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "名詞     12\n",
       "助詞     10\n",
       "記号      7\n",
       "動詞      6\n",
       "助動詞     4\n",
       "接頭詞     1\n",
       "副詞      1\n",
       "Name: 品詞, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "test.loc[:,\"nested_utters\"][0].loc[:,\"parsed_nested_utters\"][0].loc[:,\"品詞細分類1\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "固有名詞      7\n",
       "非自立       4\n",
       "句点        4\n",
       "自立        3\n",
       "終助詞       3\n",
       "接続助詞      2\n",
       "一般        2\n",
       "副詞化       1\n",
       "助詞類接続     1\n",
       "係助詞       1\n",
       "括弧閉       1\n",
       "サ変接続      1\n",
       "名詞接続      1\n",
       "形容動詞語幹    1\n",
       "括弧開       1\n",
       "並立助詞      1\n",
       "読点        1\n",
       "連体化       1\n",
       "格助詞       1\n",
       "Name: 品詞細分類1, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import re\n",
    "\n",
    "names = \"行商人 アルビン|アル(ビン)*|行商人|商人|村長 ヴァルター|そんちょ|村長|ヴァル(ター)*|仕立て屋 エルナ|エルナ*|エレナ|エ*ルナ|パン屋 オットー|オットー*|オト|パン屋|羊飼い カタリナ|カタ[(リナ)(りん)]*|(カタ)*リナ|羊飼い|司書 クララ|クララ*|ク*ララ|司書|楽天家 ゲルト|ゲルト*|楽天家|神父 ジムゾン|ジム(ゾン)*|神父|負傷兵 シモン|シモン*|兵隊|負傷兵|ならず者 ディーター|Ｄ太|ディー*タ*ー*|ならず者|木こり トーマス|トー*マス*|トム|機関車トー○ス|肉妖精|木こり|旅人 ニコラス|スナフキン|薄緑|旅人|ニコ[(ラス)(ルン)]*|村娘 パメラ|村娘|娘|パメラ*|シスター フリーデル|シスター|フリ(ーデル)*|リデル|尼|少年 ペーター|ピーター|ベーター|ペー太|ペタ|少年|ペーター|老人 モーリッツ|おじいちゃん|お爺さん|(モー)*爺|(モリ)*爺|翁|長老|老人|モーリッツ|農夫 ヤコブ|ヤコ[ブ|ビン|ぷー]*|やこびー|農夫|青年 ヨアヒム|ヨア(ヒム)*|よあひー|青年|少女 リーザ|リ[ーズ]*ザ*|少女|宿屋の女主人 レジーナ|おばさん|マダム|レジ(ーナ)*|姐さん|姐御|宿屋の女主人|女将|小母様|レジーナ\"\n",
    "\n",
    "pattern = re.compile(names)\n",
    "sent = \"ゲル,村長、そんちょ、ヴァル、※ディーター,モリ爺、モー爺、お爺さん、長老,神父、ジム,トム、トマス、トマ、機関車トー○ス、肉妖精,ニコ、スナフキン、緑、薄緑、※アルビン,ディ、ディー、ディタ、Ｄ太、※ヴァルター,ペー太、ペタ、ベーター、ピーター,リザ、リー、リズ,商人、アル、※ニコラス,カタりん、リナ、カタ,オト、オット、パン屋,ヨア、よあひー,パメ,ヤコ、ヤコぷー、ヤコビン、やこびー,レジ、女将、姐さん、姐御、おばさん、小母様、マダム,シスター、フリ、リデル,エルナ、エル、ルナ、エレナ,クララ、クラ、ララ,シモン、シモ、兵隊,楽天家 ゲルト,村長 ヴァルター,老人 モーリッツ,神父 ジムゾン,木こり トーマス,旅人 ニコラス,ならず者 ディーター,少年 ペーター,少女 リーザ,行商人 アルビン,羊飼い カタリナ,パン屋 オットー,青年 ヨアヒム,村娘 パメラ,農夫 ヤコブ,宿屋の女主人 レジーナ,シスター フリーデル,仕立て屋 エルナ,司書 クララ,負傷兵 シモン\"\n",
    "\n",
    "sent = re.sub(pattern, '<person>', sent)\n",
    "print(sent)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<person>,<person>、<person>、<person>、※<person>,<person>、<person>、<person>、<person>,<person>、<person>,<person>、<person>、<person>、<person>、<person>,<person>、<person>、緑、<person>、※<person>,<person>、<person>、<person>、<person>、※<person>,<person>、<person>、<person>、<person>,<person>、<person>、<person>,<person>、<person>、※<person>,<person>、<person>、<person>,<person>、<person>、<person>,<person>、<person>,<person>,<person>、<person>、<person>、<person>,<person>、<person>、<person>、<person>、<person>、<person>、<person>,<person>、<person>、<person>,<person>、<person>、<person>、<person>,<person>、<person>、<person>,<person>、<person>、<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>,<person>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## データを確認"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from utils.cal_stats import cal_stats\n",
    "from preprocess.tokenizer_HAN import HANtokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "test = pd.read_pickle(\"../model/data/nested_sample/test.pkl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "test.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nested_utters</th>\n",
       "      <th>num_utters</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw_neste...</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raw_nested...</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raw_nested...</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raw_neste...</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raw_nested...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       nested_utters  num_utters  label\n",
       "0                                       raw_neste...         104      1\n",
       "1                                      raw_nested...          65      1\n",
       "2                                      raw_nested...          80      1\n",
       "3                                       raw_neste...         111      0\n",
       "4                                      raw_nested...          67      0"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "nested_utters = test.loc[:,'nested_utters']\n",
    "sent = []\n",
    "for nu in nested_utters:\n",
    "    r = nu.loc[:, \"raw_nested_utters\"]\n",
    "    sent.extend(r.to_list())\n",
    "labels = test.loc[:,\"label\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "tokenizer = HANtokenizer(split_train_txt='../tokenizer/split_train.txt',\n",
    "                                    cache='../tokenizer/dim_200/',\n",
    "                                    vocab_size=32000,\n",
    "                                    min_freq=5,\n",
    "                                )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "stats, sents_tokens_to_check = cal_stats(sent, labels, tokenizer)\n",
    "with open('check.txt', 'w') as f:\n",
    "    f.write(stats)\n",
    "    f.write(\"/n/n\")\n",
    "    for st in sents_tokens_to_check:\n",
    "        f.write(st)\n",
    "        f.write(\"\\n\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train.pyの作成"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import pandas as pd\n",
    "import glob\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from model.HAN import HierAttnNet\n",
    "from model.HANDataModule import CreateHANDataModule\n",
    "from preprocess.tokenizer_HAN import HANtokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "N_EPOCHS = 5\n",
    "batch_size = 32\n",
    "vocab_size=32000\n",
    "word_hidden_dim=150\n",
    "sent_hidden_dim=150\n",
    "padding_idx=1\n",
    "embed_dim=200\n",
    "\n",
    "tokenizer = HANtokenizer(vocab_size=vocab_size)\n",
    "\n",
    "pl.seed_everything(111)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 111\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_df = pd.read_pickle('../model/data/nested/train.pkl')\n",
    "valid_df = pd.read_pickle('../model/data/nested/valid.pkl')\n",
    "test_df = pd.read_pickle('../model/data/nested/test.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "checkpoints_dir = \"./checkpoints/HAN\"\n",
    "log_dir = \"./lightning_logs/HAN\"\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data_module = CreateHANDataModule(train_df, valid_df, test_df, batch_size=batch_size, tokenizer=tokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = HierAttnNet(vocab_size=vocab_size, word_hidden_dim=word_hidden_dim, sent_hidden_dim=sent_hidden_dim, padding_idx=padding_idx,\n",
    "                        embed_dim=embed_dim, embedding_matrix=tokenizer.embedding_matrix\n",
    "                    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.005,\n",
    "    patience=3,\n",
    "    mode='min',\n",
    "    check_on_train_epoch_end=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=log_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpoints_dir,\n",
    "    filename='{epoch}',\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "trainer = pl.Trainer(max_epochs=N_EPOCHS,\n",
    "                        gpus=\"6\",\n",
    "                        precision=16,\n",
    "                        progress_bar_refresh_rate=10,\n",
    "                        callbacks=[checkpoint_callback, early_stop_callback],\n",
    "                        logger=tb_logger,\n",
    "                        plugins=DDPPlugin(find_unused_parameters=False)\n",
    "                        )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/haoki/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/pytorch_lightning/utilities/device_parser.py:135: LightningDeprecationWarning: Parsing of the Trainer argument gpus='6' (string) will change in the future. In the current version of Lightning, this will select CUDA device with index 6, but from v1.5 it will select gpus [0, 1, 2, 3, 4, 5] (same as gpus=6 (int)).\n",
      "  rank_zero_deprecation(\n",
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "trainer.fit(model=model, datamodule=data_module)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/haoki/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:101: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\n",
      "Global seed set to 111\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/home/haoki/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/torch/cuda/__init__.py:106: UserWarning: \n",
      "A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the A100-PCIE-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | wordattnnet  | WordAttnNet      | 6.8 M \n",
      "1 | sentattennet | SentAttnNet      | 497 K \n",
      "2 | ld           | Dropout          | 0     \n",
      "3 | fc           | Linear           | 602   \n",
      "4 | criterion    | CrossEntropyLoss | 0     \n",
      "--------------------------------------------------\n",
      "7.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.3 M     Total params\n",
      "29.222    Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                           "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 111\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0:   0%|          | 0/117 [00:00<00:00, 461.27it/s]  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/haoki/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:397: LightningDeprecationWarning: One of the returned values {'batch_preds', 'batch_labels'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  warning_cache.deprecation(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0: 100%|██████████| 117/117 [09:14<00:00,  4.70s/it, loss=0.695, v_num=25]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 0, global step 116: val_loss was not in top 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1:   0%|          | 0/117 [00:00<00:00, 448.97it/s, loss=0.695, v_num=25] "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1:  94%|█████████▍| 110/117 [08:48<00:33,  4.76s/it, loss=0.691, v_num=25]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: 100%|██████████| 117/117 [09:32<00:00,  4.85s/it, loss=0.689, v_num=25]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1, global step 233: val_loss was not in top 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2:   0%|          | 0/117 [00:00<00:00, 303.63it/s, loss=0.689, v_num=25] "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2:  94%|█████████▍| 110/117 [08:50<00:33,  4.78s/it, loss=0.669, v_num=25]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2: 100%|██████████| 117/117 [09:30<00:00,  4.83s/it, loss=0.648, v_num=25]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 2, global step 350: val_loss was not in top 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3:   0%|          | 0/117 [00:00<00:00, 234.91it/s, loss=0.648, v_num=25] "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3:  94%|█████████▍| 110/117 [08:45<00:33,  4.74s/it, loss=0.626, v_num=25]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3: 100%|██████████| 117/117 [09:25<00:00,  4.79s/it, loss=0.61, v_num=25] "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 3, global step 467: val_loss was not in top 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4:   0%|          | 0/117 [00:00<00:00, 396.59it/s, loss=0.61, v_num=25] "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4:  94%|█████████▍| 110/117 [08:52<00:33,  4.79s/it, loss=0.39, v_num=25] "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4: 100%|██████████| 117/117 [09:29<00:00,  4.83s/it, loss=0.406, v_num=25]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 4, global step 584: val_loss was not in top 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4: 100%|██████████| 117/117 [09:29<00:00,  4.83s/it, loss=0.406, v_num=25]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print('/disk/ssd14tb/haoki/Documents/vscode-workplaces/lie_detector/scripts'.replace('scripts', 'model'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/disk/ssd14tb/haoki/Documents/vscode-workplaces/lie_detector/model\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(trainer.checkpoint_callback.best_model_score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(dir(trainer))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['_TrainerCallbackHookMixin__is_old_signature_on_load_checkpoint', '_TrainerCallbackHookMixin__is_old_signature_on_save_checkpoint', '_Trainer__init_profiler', '_Trainer__load_ckpt_weights', '_Trainer__setup_profiler', '__abstractmethods__', '__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_active_loop', '_add_sampler_metadata_collate', '_call_configure_sharded_model', '_call_setup_hook', '_call_teardown_hook', '_default_root_dir', '_device_type', '_dispatch', '_distrib_type', '_evaluation_loop', '_fit_loop', '_get_distributed_sampler', '_is_data_prepared', '_lightning_optimizers', '_log_api_event', '_log_device_info', '_log_hyperparams', '_on_expection', '_parse_devices', '_post_dispatch', '_pre_dispatch', '_pre_training_routine', '_predict_loop', '_progress_bar_callback', '_reset_eval_dataloader', '_resolve_batch_sampler', '_results', '_run', '_run_evaluate', '_run_predict', '_run_sanity_check', '_run_train', '_setup_on_init', '_should_reload_dl_epoch', '_stochastic_weight_avg', '_test_loop', '_validate_loop', '_weights_save_path', '_worker_check', 'accelerator', 'accelerator_connector', 'accumulate_grad_batches', 'accumulation_scheduler', 'add_argparse_args', 'amp_backend', 'auto_add_sampler', 'auto_add_worker_init_fn', 'auto_lr_find', 'auto_scale_batch_size', 'call_hook', 'callback_connector', 'callback_metrics', 'callbacks', 'check_val_every_n_epoch', 'checkpoint_callback', 'checkpoint_callbacks', 'checkpoint_connector', 'config_validator', 'configure_schedulers', 'configure_sharded_model', 'convert_to_lightning_optimizers', 'current_epoch', 'data_connector', 'data_parallel', 'data_parallel_device_ids', 'datamodule', 'debugging_connector', 'default_attributes', 'default_root_dir', 'detect_nan_tensors', 'dev_debugger', 'devices', 'disable_validation', 'distributed_backend', 'distributed_sampler_kwargs', 'early_stopping_callback', 'early_stopping_callbacks', 'enable_validation', 'evaluating', 'fast_dev_run', 'fit', 'fit_loop', 'flush_logs_every_n_steps', 'from_argparse_args', 'get_deprecated_arg_names', 'global_rank', 'global_step', 'gpus', 'gradient_clip_algorithm', 'gradient_clip_val', 'has_arg', 'init_optimizers', 'interrupted', 'ipus', 'is_function_implemented', 'is_global_zero', 'is_last_batch', 'lightning_module', 'lightning_optimizers', 'limit_predict_batches', 'limit_test_batches', 'limit_train_batches', 'limit_val_batches', 'local_rank', 'log_dir', 'log_every_n_steps', 'logged_metrics', 'logger', 'logger_connector', 'lr_schedulers', 'match_env_arguments', 'max_epochs', 'max_steps', 'metrics_to_scalars', 'min_epochs', 'min_steps', 'model', 'model_connector', 'move_metrics_to_cpu', 'node_rank', 'num_gpus', 'num_nodes', 'num_predict_batches', 'num_processes', 'num_sanity_val_batches', 'num_sanity_val_steps', 'num_test_batches', 'num_training_batches', 'num_val_batches', 'on_after_backward', 'on_batch_end', 'on_batch_start', 'on_before_accelerator_backend_setup', 'on_before_backward', 'on_before_optimizer_step', 'on_before_zero_grad', 'on_epoch_end', 'on_epoch_start', 'on_fit_end', 'on_fit_start', 'on_init_end', 'on_init_start', 'on_keyboard_interrupt', 'on_load_checkpoint', 'on_predict_batch_end', 'on_predict_batch_start', 'on_predict_end', 'on_predict_epoch_end', 'on_predict_epoch_start', 'on_predict_start', 'on_pretrain_routine_end', 'on_pretrain_routine_start', 'on_sanity_check_end', 'on_sanity_check_start', 'on_save_checkpoint', 'on_test_batch_end', 'on_test_batch_start', 'on_test_end', 'on_test_epoch_end', 'on_test_epoch_start', 'on_test_start', 'on_train_batch_end', 'on_train_batch_start', 'on_train_end', 'on_train_epoch_end', 'on_train_epoch_start', 'on_train_start', 'on_validation_batch_end', 'on_validation_batch_start', 'on_validation_end', 'on_validation_epoch_end', 'on_validation_epoch_start', 'on_validation_start', 'optimizer_connector', 'optimizer_frequencies', 'optimizers', 'overfit_batches', 'overriden_optimizer_step', 'overriden_optimizer_zero_grad', 'parse_argparser', 'precision', 'precision_plugin', 'predict', 'predict_loop', 'predicted_ckpt_path', 'predicting', 'prediction_writer_callbacks', 'prepare_data_per_node', 'print_nan_gradients', 'profiler', 'progress_bar_callback', 'progress_bar_dict', 'progress_bar_metrics', 'reload_dataloaders_every_n_epochs', 'replace_sampler', 'request_dataloader', 'reset_predict_dataloader', 'reset_test_dataloader', 'reset_train_dataloader', 'reset_train_val_dataloaders', 'reset_val_dataloader', 'resume_from_checkpoint', 'root_gpu', 'run_stage', 'running_sanity_check', 'sanity_checking', 'save_checkpoint', 'scaler', 'setup', 'should_rank_save_checkpoint', 'should_stop', 'shown_warnings', 'slurm_connector', 'slurm_job_id', 'state', 'teardown', 'terminate_on_nan', 'test', 'test_dataloaders', 'test_loop', 'tested_ckpt_path', 'testing', 'tpu_cores', 'track_grad_norm', 'train_dataloader', 'train_loop', 'training', 'training_tricks_connector', 'training_type_plugin', 'truncated_bptt_steps', 'tune', 'tuner', 'tuning', 'use_amp', 'val_check_batch', 'val_check_interval', 'val_dataloaders', 'validate', 'validate_loop', 'validated_ckpt_path', 'validating', 'verbose_evaluate', 'weights_save_path', 'weights_summary', 'world_size']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "checkpoint_callback.best_model_path\n",
    "#trainer.test(ckpt_path=checkpoint_callback.best_model_path)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.test(ckpt_path=checkpoint_callback.best_model_path)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "MisconfigurationException",
     "evalue": "`.test()` found no path for the best weights: \"\". Please specify a path for a checkpoint `.test(ckpt_path=PATH)`",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36254/3078725960.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule, test_dataloaders)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtested_ckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load_ckpt_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# run test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__load_ckpt_weights\u001b[0;34m(self, ckpt_path)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m             raise MisconfigurationException(\n\u001b[0m\u001b[1;32m   1151\u001b[0m                 \u001b[0;34mf'`.{fn}()` found no path for the best weights: \"{ckpt_path}\". Please'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0;34mf\" specify a path for a checkpoint `.{fn}(ckpt_path=PATH)`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `.test()` found no path for the best weights: \"\". Please specify a path for a checkpoint `.test(ckpt_path=PATH)`"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 発話のduplicationを削除が上手くいっているか検証"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "a = ['こんにちは', 'こんにちは', 'hello']\n",
    "a = list(set(a))\n",
    "print(a)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['hello', 'こんにちは']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HANの中身"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### word atten net"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "class AttentionWithContext(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionWithContext, self).__init__()\n",
    "\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.contx = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # The first expression in the attention mechanism is simply a linear layer that receives \n",
    "        # the output of the Word-GRU referred here as 'inp' and h_{it} in the paper\n",
    "        u = torch.tanh_(self.attn(inp))\n",
    "        # The second expression is...the same but without bias, wrapped up in a Softmax function\n",
    "        a = F.softmax(self.contx(u), dim=1)\n",
    "        # And finally, an element-wise multiplication taking advantage of Pytorch's broadcasting abilities \n",
    "        s = (a * inp).sum(1)\n",
    "        # we will also return the normalized importance weights\n",
    "        return a.permute(0, 2, 1), s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, seq_len):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.weight = nn.Parameter(nn.init.kaiming_normal_(torch.Tensor(hidden_dim, 1)))\n",
    "        self.bias = nn.Parameter(torch.zeros(seq_len))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # 1. Matrix Multiplication\n",
    "        x = inp.contiguous().view(-1, self.hidden_dim)\n",
    "        u = torch.tanh_(torch.mm(x, self.weight).view(-1, self.seq_len) + self.bias)\n",
    "        # 2. Softmax on 'u_{it}' directly\n",
    "        a = F.softmax(u, dim=1)\n",
    "        # 3. Braodcasting and out\n",
    "        s = (inp * torch.unsqueeze(a, 2)).sum(1)\n",
    "        return a, s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "class WordAttnNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        hidden_dim=32,\n",
    "        padding_idx=1,\n",
    "        embed_dim=50,\n",
    "        embedding_matrix=None,\n",
    "    ):\n",
    "        super(WordAttnNet, self).__init__()\n",
    "\n",
    "        if isinstance(embedding_matrix, np.ndarray):\n",
    "            self.word_embed = nn.Embedding(\n",
    "                vocab_size, embedding_matrix.shape[1], padding_idx=padding_idx\n",
    "            )\n",
    "            self.word_embed.weight = nn.Parameter(torch.Tensor(embedding_matrix))\n",
    "            embed_dim = embedding_matrix.shape[1]\n",
    "        else:\n",
    "            self.word_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.word_attn = AttentionWithContext(hidden_dim * 2)\n",
    "\n",
    "    def forward(self, X, h_n):\n",
    "        embed = self.word_embed(X.long())\n",
    "        h_t, h_n = self.rnn(embed, h_n)\n",
    "        a, s = self.word_attn(h_t)\n",
    "        return a, s.unsqueeze(1), h_n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "bsz = 16\n",
    "maxlen_sent = 20\n",
    "hidden_dim  = 32\n",
    "embed_dim   = 100\n",
    "vocab_size  = 1000\n",
    "padding_idx = 1\n",
    "\n",
    "# net\n",
    "word_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "rnn = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "attn = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
    "contx = nn.Linear(hidden_dim*2, 1, bias=False)\n",
    "\n",
    "# inputs\n",
    "X = torch.from_numpy(np.random.choice(vocab_size, (bsz, maxlen_sent)))\n",
    "h_n = torch.zeros((2, bsz, hidden_dim))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16, 20])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 1. Word Embeddings\n",
    "# (bsz, maxlen_sent, embed_dim)\n",
    "embed = word_embed(X)\n",
    "embed.shape # (bsz, maxlen_sent, emb_dim)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16, 20, 100])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 2. GRU\n",
    "h_t, h_n = rnn(embed, h_n)\n",
    "# (bsz, seq_len, hidden_dim*2)\n",
    "h_t.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16, 20, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# 3. Attention\n",
    "u = torch.tanh_(attn(h_t))\n",
    "cu = contx(u)\n",
    "a = F.softmax(cu, dim=1) # 全単語に対して重みを計算\n",
    "print(h_t.shape, u.shape, cu.shape, a.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([16, 20, 64]) torch.Size([16, 20, 64]) torch.Size([16, 20, 1]) torch.Size([16, 20, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "contx.weight.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# RNN outputs scaled by their importance weights\n",
    "s = (a * h_t) # 単語の隠れ状態の重み付き和が文のベクトルsになる。\n",
    "print(s.shape)\n",
    "# Sum along the seq dim so we end up with a representation per document/review\n",
    "s = s.sum(1)\n",
    "print(s.shape)\n",
    "# Because this will be stack for all sentences, we do the `.unsqueeze(1)`\n",
    "print(s.unsqueeze(1).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([16, 20, 64])\n",
      "torch.Size([16, 64])\n",
      "torch.Size([16, 1, 64])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "f = torch.ones(2,3)\n",
    "print(f)\n",
    "print(f.unsqueeze(1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[[1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### sent attn net"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "class SentAttnNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, word_hidden_dim=32, sent_hidden_dim=32, padding_idx=1\n",
    "    ):\n",
    "        super(SentAttnNet, self).__init__()\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            word_hidden_dim * 2, sent_hidden_dim, bidirectional=True, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.sent_attn = AttentionWithContext(sent_hidden_dim * 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        h_t, h_n = self.rnn(X)\n",
    "        a, v = self.sent_attn(h_t)\n",
    "        return a.permute(0,2,1), v"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### han"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "class HierAttnNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        maxlen_sent,\n",
    "        maxlen_doc,\n",
    "        word_hidden_dim=32,\n",
    "        sent_hidden_dim=32,\n",
    "        padding_idx=1,\n",
    "        embed_dim=50,\n",
    "        embedding_matrix=None,\n",
    "        num_class=4,\n",
    "    ):\n",
    "        super(HierAttnNet, self).__init__()\n",
    "\n",
    "        self.word_hidden_dim = word_hidden_dim\n",
    "\n",
    "        self.wordattnnet = WordAttnNet(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_dim=word_hidden_dim,\n",
    "            padding_idx=padding_idx,\n",
    "            embed_dim=embed_dim,\n",
    "            embedding_matrix=embedding_matrix,\n",
    "        )\n",
    "\n",
    "        self.sentattnnet = SentAttnNet(\n",
    "            word_hidden_dim=word_hidden_dim,\n",
    "            sent_hidden_dim=sent_hidden_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(sent_hidden_dim * 2, num_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = X.permute(1, 0, 2)\n",
    "        word_h_n = nn.init.zeros_(torch.Tensor(2, X.shape[0], self.word_hidden_dim))\n",
    "        if use_cuda:\n",
    "            word_h_n = word_h_n.cuda()\n",
    "        # alpha and s Tensor Lists\n",
    "        word_a_list, word_s_list = [], []\n",
    "        for sent in x:\n",
    "            word_a, word_s, word_h_n = self.wordattnnet(sent, word_h_n)\n",
    "            word_a_list.append(word_a)\n",
    "            word_s_list.append(word_s)\n",
    "        # Importance attention weights per word in sentence\n",
    "        self.sent_a = torch.cat(word_a_list, 1)\n",
    "        # Sentences representation\n",
    "        sent_s = torch.cat(word_s_list, 1)\n",
    "        # Importance attention weights per sentence in doc and document representation\n",
    "        self.doc_a, doc_s = self.sentattnnet(sent_s)\n",
    "        return self.fc(doc_s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "maxlen_sent = 20\n",
    "maxlen_doc = 5\n",
    "num_class = 4\n",
    "word_hidden_dim = 32\n",
    "sent_hidden_dim = 32\n",
    "\n",
    "wordattnnet = WordAttnNet(vocab_size, hidden_dim, padding_idx, embed_dim, embedding_matrix=None)\n",
    "sentattnnet = SentAttnNet(word_hidden_dim, sent_hidden_dim, padding_idx)\n",
    "fc = nn.Linear(sent_hidden_dim * 2, num_class)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'WordAttnNet' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_61382/3683367917.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msent_hidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwordattnnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordAttnNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0msentattnnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentAttnNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_hidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_hidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_hidden_dim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordAttnNet' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X = torch.from_numpy(np.random.choice(vocab_size, (bsz, maxlen_doc, maxlen_sent)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "a = [961, 626, 368, 545,  76,  57, 609, 835, 773, 544, 776, 623,  80, 758, 617, 527, 581, 592, 432, 445]\n",
    "b = torch.tensor(a)\n",
    "print(b)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([961, 626, 368, 545,  76,  57, 609, 835, 773, 544, 776, 623,  80, 758,\n",
      "        617, 527, 581, 592, 432, 445])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(X[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[961, 626, 368, 545,  76,  57, 609, 835, 773, 544, 776, 623,  80, 758,\n",
      "         617, 527, 581, 592, 432, 445],\n",
      "        [ 21, 603,  33, 201, 136, 819, 933, 178, 610,  43,  64, 564, 570, 384,\n",
      "         784, 942, 967, 182, 774, 675],\n",
      "        [113, 722,  25, 805, 676, 237, 139, 820, 259, 500, 821, 932, 902, 542,\n",
      "         783, 895, 155, 973, 737, 699],\n",
      "        [324, 480, 342, 372, 205, 772, 362, 428, 917, 910, 135, 492, 480, 998,\n",
      "         258, 955, 418, 535, 289, 652],\n",
      "        [995, 440, 408, 548,  82, 389, 481, 906, 925, 966, 809, 215, 165, 421,\n",
      "         943, 529, 972, 156, 248, 580]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "x = X.permute(1, 0, 2)\n",
    "x.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 20])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Initial Word RNN hidden state\n",
    "word_h_n = nn.init.zeros_(torch.Tensor(2, X.shape[0], word_hidden_dim))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# Loop through sentences:\n",
    "word_a_list, word_s_list = [], []\n",
    "for sent in x:\n",
    "    print(sent.shape)\n",
    "    word_a, word_s, word_h_n = wordattnnet(sent, word_h_n)\n",
    "    word_a_list.append(word_a)\n",
    "    word_s_list.append(word_s)\n",
    "# Importance attention weights per word in sentence\n",
    "sent_a = torch.cat(word_a_list, 1)\n",
    "# Sentences representation\n",
    "sent_s = torch.cat(word_s_list, 1)\n",
    "# (bsz, maxlen_doc, maxlen_sent)\n",
    "print(sent_a.shape)\n",
    "# (bsz, maxlen_doc, hidden_dim*2)\n",
    "print(sent_s.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([16, 20])\n",
      "torch.Size([16, 20])\n",
      "torch.Size([16, 20])\n",
      "torch.Size([16, 20])\n",
      "torch.Size([16, 20])\n",
      "torch.Size([16, 5, 20])\n",
      "torch.Size([16, 5, 64])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "doc_a, doc_s = sentattnnet(sent_s)\n",
    "# (bsz, maxlen_doc, 1). One could .squeeze(2)\n",
    "print(doc_a.shape)\n",
    "# (bsz, hidden_dim*2)\n",
    "print(doc_s.shape) # docの隠れ状態"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([16, 5, 1])\n",
      "torch.Size([16, 64])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "out = fc(doc_s) # (bsz, class_num)\n",
    "out.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataModule.pyが正しく動いているかチェック\n",
    "⇒同時に、HANに入れる前にどうしていたかもチェック\n",
    "⇒ちゃんとできてそうだった！"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.accelerators import accelerator\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.accelerators.gpu import GPUAccelerator\n",
    "from pytorch_lightning.plugins import NativeMixedPrecisionPlugin"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)\n",
    "from model.HANDataModule import CreateHANDataModule\n",
    "from preprocess.tokenizer_HAN import HANtokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_pickle('../model/data/nested/train.pkl')\n",
    "valid_df = pd.read_pickle('../model/data/nested/valid.pkl')\n",
    "test_df = pd.read_pickle('../model/data/nested/test.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "pl.seed_everything(111)\n",
    "data_module = CreateHANDataModule(train_df, valid_df, test_df, batch_size=5, tokenizer=HANtokenizer())\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 111\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "data_module.setup(stage='test')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "test_ds = data_module.test_ds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "len(test_ds.__getitem__(0).get('nested_utters'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "_test_dl = DataLoader(dataset=test_ds, batch_size=5, shuffle=False, pin_memory=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "print(len(_test_dl))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "94\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "x,y = next(iter(_test_dl)).values()\n",
    "print(x)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[  25,    8,    5,  ...,    1,    1,    1],\n",
      "         [ 753,    5,  101,  ...,    1,    1,    1],\n",
      "         [1173,    5,  120,  ...,    1,    1,    1],\n",
      "         ...,\n",
      "         [   1,    1,    1,  ...,    1,    1,    1],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1]],\n",
      "\n",
      "        [[ 162,   82,   56,  ...,    1,    1,    1],\n",
      "         [   8,  297,   58,  ...,    1,    1,    1],\n",
      "         [   8,    9, 7743,  ...,    1,    1,    1],\n",
      "         ...,\n",
      "         [   1,    1,    1,  ...,    1,    1,    1],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1]],\n",
      "\n",
      "        [[ 775,    8,    9,  ...,    1,    1,    1],\n",
      "         [ 163,    5,  940,  ...,    1,    1,    1],\n",
      "         [ 220,  220,    5,  ...,    1,    1,    1],\n",
      "         ...,\n",
      "         [   1,    1,    1,  ...,    1,    1,    1],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1]],\n",
      "\n",
      "        [[ 162,    8,  199,  ...,    1,    1,    1],\n",
      "         [ 162,    8,    9,  ...,    1,    1,    1],\n",
      "         [ 167,  451,  169,  ...,    1,    1,    1],\n",
      "         ...,\n",
      "         [   1,    1,    1,  ...,    1,    1,    1],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1]],\n",
      "\n",
      "        [[6146,  116,    8,  ...,    1,    1,    1],\n",
      "         [  95,    6,  166,  ...,    1,    1,    1],\n",
      "         [1586,    9,  151,  ...,    1,    1,    1],\n",
      "         ...,\n",
      "         [   1,    1,    1,  ...,    1,    1,    1],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1],\n",
      "         [   1,    1,    1,  ...,    1,    1,    1]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "test_dl = data_module.test_dataloader()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/haoki/Documents/vscode-workplaces/lie_detector/venv/lib/python3.9/site-packages/torch/cuda/__init__.py:106: UserWarning: \n",
      "A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the A100-PCIE-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "len(test_dl)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "test_batch = next(iter(test_dl))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(test_batch)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nested_utters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pandasを思ったように使えているか実験(preprocess_HAN.py, HANDataModule.py)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "padding = [[1 for _ in range(10)] for _ in range(2)]\n",
    "print(padding)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "sys.path.append(os.pardir)\n",
    "from preprocess.tokenizer_HAN import HANtokenizer\n",
    "\n",
    "index = 0\n",
    "\n",
    "df = pd.read_pickle('../model/data/nested/test.pkl')\n",
    "df_row = df.loc[:,'nested_utters'].iloc[index]['raw_nested_utters']\n",
    "label = df.loc[:,'label'].iloc[index]\n",
    "\n",
    "tokenizer = HANtokenizer(split_train_txt='../tokenizer/split_train.txt',\n",
    "                            cache='../tokenizer/dim_200/',\n",
    "                            vocab_size=32000,\n",
    "                            min_freq=5,\n",
    "                        )\n",
    "print(df_row.head())\n",
    "print(label)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    おー、始まってるな。（え、英語は…苦手だ）<person>、寝てばっかりいないで、たまには起...\n",
      "1    お？まずは来れる時間か？俺は日によって違うから何とも言えねぇから、プロローグの通り、「夕方～...\n",
      "2    …俺も、にんにく好きだ……。ただし、火の通ったものに限る。俺も占い師６人の村に訪れてみたいぜ...\n",
      "3    例えかよっ！？＞<person>そうそう、霊能者＆占い師が潜伏するにしても、日目にはＣＯとか...\n",
      "4    <person>、２日目にＣＯは早いと思うぜ。それじゃあ、潜伏の意味があまりない。で、俺の意...\n",
      "Name: raw_nested_utters, dtype: object\n",
      "1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "padded_nested_utters = tokenizer.encode(df_row)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(padded_nested_utters.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[117, 147,   5,  ...,   1,   1,   1],\n",
      "        [117,  31, 370,  ...,   1,   1,   1],\n",
      "        [ 25,  91,  19,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ones = '1 '*10\n",
    "print(list(map(int, ones.split())))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "padded_nested_utters.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "padded_nested_utters[0][0][0][0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df_row_tokenized = df_row.apply(preproceser.tokenize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df_row_numericalized = df_row_tokenized.apply(preproceser.numericalize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(type(df_row_numericalized[0][0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "word_padded_df = df_row_numericalized.apply(preproceser.padding_word_level)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch\n",
    "word_padded = [torch.tensor(utter) for utter in word_padded_df.to_list()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "padding = [torch.ones_like(word_padded[0]) for _ in range(20)]\n",
    "print(len(word_padded))\n",
    "padded = word_padded + padding\n",
    "print(len(padded))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "104\n",
      "124\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "padded_nested_utters = preproceser.padding_sent_level(word_padded)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "a = torch.tensor(padded_nested_utters)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66352/97098321.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_nested_utters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "padded_nested_utters[0][0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "pandasの実験（くっ付ける）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = [1,2,3,4,5,6,7,8]\n",
    "b = [9,10,11,12,13]\n",
    "\n",
    "a_df = pd.DataFrame({'utter': a})\n",
    "b_df = pd.DataFrame({'utter': b})\n",
    "\n",
    "c = pd.concat([a_df, b_df], axis=0)\n",
    "print(c)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "tokenize⇒numericalize⇒embeddig matrix作成までの実験(preprocecc_HAN)のため。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torchtext.vocab import Vocab\n",
    "from collections import Counter, OrderedDict\n",
    "from typing import Dict, List, Optional, Iterable\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "def vocab_with_vocab_size(ordered_dict: Dict, min_freq: int = 1, vocab_size: int = 32000) -> Vocab:\n",
    "    r\"\"\"Factory method for creating a vocab object which maps tokens to indices.\n",
    "\n",
    "    Note that the ordering in which key value pairs were inserted in the `ordered_dict` will be respected when building the vocab.\n",
    "    Therefore if sorting by token frequency is important to the user, the `ordered_dict` should be created in a way to reflect this.\n",
    "\n",
    "    Args:\n",
    "        ordered_dict: Ordered Dictionary mapping tokens to their corresponding occurance frequencies.\n",
    "        min_freq: The minimum frequency needed to include a token in the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        torchtext.vocab.Vocab: A `Vocab` object\n",
    "\n",
    "    Examples:\n",
    "        >>> from torchtext.vocab import vocab\n",
    "        >>> from collections import Counter, OrderedDict\n",
    "        >>> counter = Counter([\"a\", \"a\", \"b\", \"b\", \"b\"])\n",
    "        >>> sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        >>> ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "        >>> v1 = vocab(ordered_dict)\n",
    "        >>> print(v1['a']) #prints 1\n",
    "        >>> print(v1['out of vocab']) #raise RuntimeError since default index is not set\n",
    "        >>> tokens = ['e', 'd', 'c', 'b', 'a']\n",
    "        >>> v2 = vocab(OrderedDict([(token, 1) for token in tokens]))\n",
    "        >>> #adding <unk> token and default index\n",
    "        >>> unk_token = '<unk>'\n",
    "        >>> default_index = -1\n",
    "        >>> if unk_token not in v2: v2.insert_token(unk_token, 0)\n",
    "        >>> v2.set_default_index(default_index)\n",
    "        >>> print(v2['<unk>']) #prints 0\n",
    "        >>> print(v2['out of vocab']) #prints -1\n",
    "        >>> #make default index same as index of unk_token\n",
    "        >>> v2.set_default_index(v2[unk_token])\n",
    "        >>> v2['out of vocab'] is v2[unk_token] #prints True\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = []\n",
    "    for token, freq in ordered_dict.items():\n",
    "        if freq >= min_freq:\n",
    "            tokens.append(token)\n",
    "        if len(tokens) > vocab_size:\n",
    "            break\n",
    "\n",
    "    return Vocab(VocabPybind(tokens, None))\n",
    "\n",
    "\n",
    "def build_vocab_from_iterator_with_vocab_size(iterator: Iterable, min_freq: int = 1, vocab_size: int = 32000, specials: Optional[List[str]] = None, special_first: bool = True) -> Vocab:\n",
    "    \"\"\"\n",
    "    Build a Vocab from an iterator.\n",
    "\n",
    "    Args:\n",
    "        iterator: Iterator used to build Vocab. Must yield list or iterator of tokens.\n",
    "        min_freq: The minimum frequency needed to include a token in the vocabulary.\n",
    "        specials: Special symbols to add. The order of supplied tokens will be preserved.\n",
    "        special_first: Indicates whether to insert symbols at the beginning or at the end.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        torchtext.vocab.Vocab: A `Vocab` object\n",
    "\n",
    "    Examples:\n",
    "        >>> #generating vocab from text file\n",
    "        >>> import io\n",
    "        >>> from torchtext.vocab import build_vocab_from_iterator\n",
    "        >>> def yield_tokens(file_path):\n",
    "        >>>     with io.open(file_path, encoding = 'utf-8') as f:\n",
    "        >>>         for line in f:\n",
    "        >>>             yield line.strip().split()\n",
    "        >>> vocab = build_vocab_from_iterator(yield_tokens_batch(file_path), specials=[\"<unk>\"])\n",
    "    \"\"\"\n",
    "\n",
    "    counter = Counter()\n",
    "    for tokens in iterator:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    if specials is not None:\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "    sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[0])\n",
    "    sorted_by_freq_tuples.sort(key=lambda x: x[1], reverse=True)\n",
    "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "    if specials is not None:\n",
    "        if special_first:\n",
    "            specials = specials[::-1]\n",
    "        for symbol in specials:\n",
    "            ordered_dict.update({symbol: min_freq})\n",
    "            ordered_dict.move_to_end(symbol, last=not special_first)\n",
    "\n",
    "    word_vocab = vocab_with_vocab_size(ordered_dict, min_freq=min_freq, vocab_size=vocab_size + len(specials))\n",
    "    return word_vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import MeCab\n",
    "\n",
    "wakati = MeCab.Tagger(\"-O wakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "def yield_tokens():\n",
    "    with open('../tokenizer/split_train.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            yield line.split()\n",
    "specials=['<unk>', '<PAD>', '<BOS>', '<EOS>']\n",
    "vocab = build_vocab_from_iterator_with_vocab_size(yield_tokens(), min_freq=5, specials=specials)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocab.lookup_token(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torchtext.vocab import Vectors\n",
    "\n",
    "vectors = Vectors(name='model_fasttext.vec',\n",
    "                        cache='../tokenizer/dim_200/'\n",
    "                        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "vocab_size = 32000\n",
    "special_tokens_matrix = torch.zeros(len(specials), 200)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stoi = vocab.get_stoi()\n",
    "sorted_stoi = dict(sorted(stoi.items(), key=lambda x: x[1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "other_tokens_matrix = vectors.get_vecs_by_tokens(list(sorted_stoi.keys())[len(specials):])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding_matrix = torch.cat((special_tokens_matrix ,other_tokens_matrix), dim=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import MeCab\n",
    "\n",
    "wakati = MeCab.Tagger(\"-O wakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "line = wakati.parse(\"私。<person>さんは【人間】だったわ。でも、<person>さんが危惧していた通りになってしまったわね・・・ご冥福をお祈り致しますわ。私も今日は少し所用があるので、本格的に顔を出せるのは夕刻以降になりそうですわ。それでは、また。\").split()\n",
    "print(line)\n",
    "indices = vocab.lookup_indices(line)\n",
    "print(indices)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding_matrix.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "形態素解析についての実験"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "valid = pd.read_pickle('../model/data/nested/valid.pickle')\n",
    "\n",
    "print(valid.head())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(valid.loc[:,'utters'].iloc[0].loc[:,'parsed_utters'].head())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(valid.loc[:,'labels'].iloc[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(valid['utters'][0]['parsed_utters'][0].head())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "texts1 = ['あああ', 'いいい', 'ううう']\n",
    "texts2 = ['えええ', 'おおお', 'かかか']\n",
    "utterances1 = pd.DataFrame({'texts1':texts1})\n",
    "print(utterances1['texts1'].to_list())\n",
    "len(utterances1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import MeCab\n",
    "wakati = MeCab.Tagger(\"-O wakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "line = wakati.parse(\"私。<person>さんは【人間】だったわ。<br>でも、レジーナさんが危惧していた通りになってしまったわね・・・ご冥福をお祈り致しますわ。私も今日は少し所用があるので、本格的に顔を出せるのは夕刻以降になりそうですわ。それでは、また。\")\n",
    "print(line.split())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "with open('../model/data/train.pickle','rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "print(f\"{train['num_utters'].mean():,.2f}\")\n",
    "\n",
    "print(train['utters'][0].head())\n",
    "\n",
    "\"\"\"\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for fold_id, (train_idx, valid_index) in enumerate(cv.split(train['utters'], train['labels'])):\n",
    "    X_tr = train['utters'][train_idx]\n",
    "    X_val = train['utters'][valid_index]\n",
    "    y_tr = train['labels'][train_idx]\n",
    "    y_val = train['labels'][valid_index]\n",
    "    print(fold_id)\n",
    "    print(X_tr)\n",
    "    print(y_tr)\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "texts1 = ['あああ', 'いいい', 'ううう']\n",
    "texts2 = ['えええ', 'おおお', 'かかか']\n",
    "utterances1 = pd.DataFrame({'texts':texts1})\n",
    "utterances2 = pd.DataFrame({'texts':texts2})\n",
    "utterances = [utterances1, utterances2]\n",
    "labels = [0, 1]\n",
    "\n",
    "df = pd.DataFrame({'utters':utterances, 'labels': labels})\n",
    "\n",
    "for t in df.get('utters'):\n",
    "    t['length'] = len(t['texts'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "\"\"\" train = []\n",
    "\n",
    "for utters, label in zip(utterances, labels):\n",
    "    train.append({'utterances': utters, 'labels': label})\n",
    "\n",
    "for t in train:\n",
    "    t.get('utterances')['length'] = len(t.get('utterances'))\n",
    "\n",
    "with open('train.pickle', 'bw') as f:\n",
    "    pickle.dump(train, f, protocol=5)\n",
    "\n",
    "with open('train.pickle', 'br') as f:\n",
    "    trainp = pickle.load(f)\n",
    "\n",
    "print(trainp) \"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torchmetrics\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def cm(preds, labels):\n",
    "    cm = torchmetrics.ConfusionMatrix(num_classes=2)\n",
    "\n",
    "    df_cm = pd.DataFrame(cm(preds, labels).numpy())\n",
    "    print(df_cm.to_string())\n",
    "\n",
    "    plt.figure(figsize=(2,2))\n",
    "    fig = sns.heatmap(df_cm, annot=True, cmap='Spectral').get_figure()\n",
    "    plt.show()\n",
    "preds = torch.tensor([0,1,1,1])\n",
    "labels = torch.tensor([0,1,0,1])\n",
    "\n",
    "cm(preds, labels)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torchmetrics import AveragePrecision, F1\n",
    "# from torchmetrics import BinnedAveragePrecision\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "pred = torch.tensor([0,1,1,1,0])\n",
    "label = torch.tensor([0,1,0,1,1])\n",
    "\n",
    "# 2/3 = 0.66...\n",
    "average_precision = AveragePrecision(pos_label=1, num_classes=2)\n",
    "# bined_precision = BinnedAveragePrecision(num_classes=1)\n",
    "f1 = F1(num_classes=2)\n",
    "f1_skl, prec, recall,_ = precision_recall_fscore_support(label, pred, average=\"micro\")\n",
    "print(f\"average precision:{average_precision(pred, label)}\")\n",
    "# print(f\"bined precision{bined_precision(pred, label)}\")\n",
    "print(f\"torch metrics{f1(pred, label)}\")\n",
    "print(f\"skl f1: {f1_skl}, prec:{prec}, recall:{recall}\")\n",
    "cm(pred, label)\n",
    "scores_df = pd.DataFrame(np.array(precision_recall_fscore_support(preds, preds)).T,\n",
    "                                    columns=[\"precision\", \"recall\", \"f1\", \"support\"],\n",
    "                                )\n",
    "fig, ax = plt.subplots(figsize=(2,2))\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "ax.table(cellText=scores_df.values,\n",
    "         colLabels=scores_df.columns,\n",
    "         loc='center')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "p=torch.Tensor([[0.9, 0.1], [0.4, 0.6]])\n",
    "print(p.argmax(dim=1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_pickle('../model/data/flat/balance/test.pkl')\n",
    "\n",
    "lie = test.query(\"label==1\")\n",
    "lie['lie'] = False\n",
    "lie['level'] = ''\n",
    "lie['reason'] = ''\n",
    "lie = lie[:5000]\n",
    "lie.to_csv('lie1.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer\n",
    "import os, sys\n",
    "sys.path.append(os.pardir)\n",
    "from model.BERT import Classifier\n",
    "pretrained_model_path='cl-tohoku/bert-base-japanese'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model_path)\n",
    "\n",
    "#model = Classifier.load_from_checkpoint('/home/lyriatest/haoki/Documents/vscode-workplaces/lie-detector/model/checkpoints/balance/epoch=3.ckpt', n_classes=2)\n",
    "tokenizer.decode\n",
    "def tokenize(text):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "    return encoding['input_ids'].flatten(), encoding['attention_mask'].flatten()\n",
    "   \n",
    "text1 = 'うん、何か白アピしてみたけど全然神父白くないな。<br>神父狼あるな！！<br><br>そんなわけで私はパメラ人だと思うので、じいさんは生きている限り、<br>羊→旅→宿or神の順に焼き払ってくれたらいいかと。<br>宿と旅を見比べたけど、襲撃筋考えると旅狼の方が濃い。だから旅先吊り。パメラは襲撃されなかったら考える。'\n",
    "text2 = ''\n",
    "input_ids = []\n",
    "atten_mask = []\n",
    "for t in [text1, text2]:\n",
    "    print(t, len(t))\n",
    "    ids = tokenizer.encode(t, add_special_tokens=False)\n",
    "    print(ids, len(ids))\n",
    "    decode = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "    print(decode, len(decode.split(' ')))\n",
    "    print()\n",
    "\n",
    "# print(torch.stack(ex, dim=1))\n",
    "# a = encoding['input_ids']\n",
    "# print(a.size())\n",
    "\n",
    "# print(model(torch.stack(input_ids, dim=0), torch.stack(atten_mask, dim=0)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "a = [1,2,3]\n",
    "\n",
    "with open('./pra.txt', 'w') as f:\n",
    "    for row in a:\n",
    "        f.write(f\"{row}\\n\")\n",
    "        print(\"yay\", file=f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from utils.cal_stats import cal_stats\n",
    "\n",
    "pretrained_model_path='cl-tohoku/bert-base-japanese'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model_path)\n",
    "\n",
    "train_df = pd.read_pickle('/home/lyriatest/haoki/Documents/vscode-workplaces/lie-detector/model/data/flat/aaa/balance/train.pkl')\n",
    "valid_df = pd.read_pickle('/home/lyriatest/haoki/Documents/vscode-workplaces/lie-detector/model/data/flat/aaa/balance/valid.pkl')\n",
    "test_df = pd.read_pickle('/home/lyriatest/haoki/Documents/vscode-workplaces/lie-detector/model/data/flat/aaa/balance/test.pkl')\n",
    "\n",
    "for name, df in [('train', train_df), ('valid', valid_df), ('test', test_df)]:\n",
    "    text = df['text'].tolist()\n",
    "    label = df['label'].tolist()\n",
    "\n",
    "    stats, sents_tokens_to_check = cal_stats(text, label, tokenizer)\n",
    "\n",
    "    with open(f'./stats_{name}.txt', 'w') as f:\n",
    "        f.write(stats)\n",
    "\n",
    "    with open(f'sents_tokens_{name}.txt', 'w') as f:\n",
    "        for l in sents_tokens_to_check:\n",
    "            f.write(l)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "pretrained_model_path='cl-tohoku/bert-base-japanese'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model_path)\n",
    "\n",
    "valid_df = pd.read_pickle('/home/lyriatest/haoki/Documents/vscode-workplaces/lie-detector/model/data/flat/balance/train.pkl')\n",
    "sentences = valid_df['text'].tolist()\n",
    "labels = valid_df['label'].tolist()\n",
    "len_count = {}\n",
    "\n",
    "for sentence, _ in zip(sentences, labels):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    sent_len = len(tokens)\n",
    "    len_count[sent_len] = len_count.get(sent_len, 0) + 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len_count = sorted(len_count.items(), key=lambda x:x[0])\n",
    "len_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}
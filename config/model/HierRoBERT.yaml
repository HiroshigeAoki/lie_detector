name: HierRoBERT

args:
  num_labels: 2
  _target_: src.model.HierRoBERT.HierchicalRoBERT
  use_ave_pooled_output: True
  output_attentions: True
  is_japanese: True

tokenizer:
  _target_: src.tokenizer.tokenizer_HierRoBERT.HierRoBertTokenizer
  sent_length: 256
  doc_length: 256
  pretrained_model: 'itsunoda/wolfbbsRoBERTa-large'

data_module:
  _target_: src.model.HierBERTDataModule.CreateHierBertDataModule
  batch_size: 64

sent_level_BERT_config:
  _target_: transformers.BertConfig
  # vocab_size: 30522 # Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the inputs_ids passed when calling BertModel or TFBertModel.
  hidden_size: 768 # Dimensionality of the encoder layers and the pooler layer.
  num_hidden_layers: 12 # Number of hidden layers in the Transformer encoder.
  num_attention_heads: 12 # Number of attention heads for each attention layer in the Transformer encoder.
  # intermediate_size: 3072 # Dimensionality of the “intermediate” (often named feed-forward) layer in the Transformer encoder.
  # hidden_act: "gelu" #The non-linear activation function (function or string) in the encoder and pooler. If string, "gelu", "relu", "silu" and "gelu_new" are supported.
  # hidden_dropout_prob: 0.1 # The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.
  # attention_probs_dropout_prob: 0.1 # The dropout ratio for the attention probabilities.
  # max_position_embeddings: 512 : The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).
  # type_vocab_size: 2 # The vocabulary size of the token_type_ids passed when calling BertModel or TFBertModel.
  # initializer_range: 0.02 # The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
  # layer_norm_eps: 1e-12 # The epsilon used by the layer normalization layers.
  # position_embedding_type: "absolute" # Type of position embedding. Choose one of "absolute", "relative_key", "relative_key_query". For positional embeddings use "absolute". For more information on "relative_key", please refer to Self-Attention with Relative Position Representations (Shaw et al.). For more information on "relative_key_query", please refer to Method 4 in Improve Transformer Models with Better Relative Position Embeddings (Huang et al.).
  # use_cache: True # Whether or not the model should return the last key/values attentions (not used by all models). Only relevant if config.is_decoder=True.
  # classifier_dropout: 0.0 # The dropout ratio for the classification head.
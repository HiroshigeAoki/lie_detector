hydra:
  run:
    dir: outputs/${data.name}/${model.name}/${experiment}/${outputdir_name}

experiment: baseline
outputdir_name: ${now:%Y-%m-%d_%H%M%S}
message:
workplace_dir: /home/haoki/Documents/vscode-workplaces/lie_detector
checkpoint_dir: null #outputs以下の相対パス ex) nested/HAN/200_dim200_sp/checkpoints

defaults:
  - _self_
  - model: HAN
  - tokenizer: sentencepiece
  - data: nested
  - optim: AdamW
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

mode: train
best_epoch: 0

# Pytorch lightning trainer's argument
# default flags are commented to avoid clustering the hyperparameters
trainer:
  # accelerator: None
  accumulate_grad_batches: 1
  # amp_backend: native
  # amp_level: O2
  # auto_lr_find: False
  # auto_scale_batch_size: False
  # auto_select_gpus: False
  benchmark: True
  # check_val_every_n_epoch: 1
  # checkpoint_callback: True
  # default_root_dir:
  deterministic: True
  fast_dev_run: False
  # flush_logs_every_n_steps: 100
  gpus: [0]
  # gradient_clip_val: 0
  # limit_predict_batches: 1.0
  limit_test_batches: 1.0
  # limit_train_batches: 1.0
  # limit_val_batches: 1.0
  # log_every_n_steps: 50
  # log_gpu_memory: False
  # logger: True
  max_epochs: 10
  # max_steps: None
  # min_epochs: None
  # min_steps: None
  # move_metrics_to_cpu: False
  # multiple_trainloader_mode: max_size_cycle
  # num_nodes: 1
  # num_processes: 1
  num_sanity_val_steps: 2
  overfit_batches: 0.0
  # plugins: None
  precision: 16
  # prepare_data_per_node: True
  # process_position: 0
  # profiler: None
  # progress_bar_refresh_rate: None
  # reload_dataloaders_every_epoch: False
  # replace_sampler_ddp: True
  # resume_from_checkpoint: None
  # stochastic_weight_avg: False
  # sync_batchnorm: False
  terminate_on_nan: True
  # track_grad_norm: -1
  # truncated_bptt_steps: None
  # val_check_interval: 1.0
  # weights_save_path: None
  # weights_summary: top

early_stopping:
  _target_ : pytorch_lightning.callbacks.EarlyStopping
  monitor: 'val_loss'
  min_delta: 0.005
  patience: 3
  mode: 'min'
  check_on_train_epoch_end: False

checkpoint_callback:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  #dirpath=checkpoints_dir
  filename: '{epoch}'
  monitor: 'val_loss'
  verbose: True
  # save_last: None
  save_top_k: 1
  mode: 'min'